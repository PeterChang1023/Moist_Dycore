{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f119f76-e7b3-461c-bf6b-592e13bf564d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import datetime \n",
    "import time\n",
    "from scipy.fft import fft, ifft, fftshift\n",
    "from scipy.fftpack import fftfreq\n",
    "from EOF import EOF\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26ed02eb-aba9-49ef-9bd0-f5158e298def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from multiprocessing import Pool\n",
    "from Zonal_Calculation import Zonal_Calculation\n",
    "import time\n",
    "from multiprocessing import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a897ae4-f7d9-46e8-81fe-c9fb090b99bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a5749b3-e459-4cc6-8288-ba3b369732a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01839787 0.07359148 0.12410124 0.17452541 0.22459845 0.27468774]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bca90242-6e2c-42d3-a4d1-51af4f674db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_prec(qv_diff, p_half):\n",
    "#     g = 9.81\n",
    "#     Prec = np.zeros(qv_diff.shape)\n",
    "#     for i in range(1, 20 - 1):\n",
    "#         Prec[:, i, :, :] = 1 / g * qv_diff[:, i, :, :] * (p_half[:, i + 1, :, :] - p_half[:, i, :, :])\n",
    "#     Prec[:, 0, :, :] = 1 / g * qv_diff[:, 0, :, :] * (p_half[:, 1, :, :] - p_half[:, 0, :, :])\n",
    "#     Prec_mean = np.nansum(Prec[:,:6,:,:], axis=(1)) # (time, y, x)\n",
    "#     # Prec_mean_y = np.nanmean(Prec_mean, axis=(0, 2))\n",
    "#     return Prec_mean\n",
    "\n",
    "# def calculate_prec_chunk(qv_diff_chunk, p_half_chunk):\n",
    "#     g = 9.81\n",
    "#     Prec_chunk = np.zeros(qv_diff_chunk.shape)\n",
    "#     for i in range(1, 20 - 1):\n",
    "#         Prec_chunk[:, i, :, :] = 1 / g * qv_diff_chunk[:, i, :, :] * (p_half_chunk[:, i + 1, :, :] - p_half_chunk[:, i, :, :])\n",
    "#     Prec_chunk[:, 0, :, :] = 1 / g * qv_diff_chunk[:, 0, :, :] * (p_half_chunk[:, 1, :, :] - p_half_chunk[:, 0, :, :])\n",
    "#     Prec_mean_chunk = np.nansum(Prec_chunk[:, :12, :, :], axis=1)  # (time, y, x)\n",
    "#     return Prec_mean_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2573d1-461c-4748-bedb-d18eb2bb98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_pr(PR):\n",
    "#     print(f\"Processing PR={PR}\")\n",
    "#     qv_diff_file_path = \"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF\" + f\"/PR{PR}/qv_diff/PR{PR}_500_20000day_6hourly_qv_diff.dat\"\n",
    "#     p_half_file_path  = \"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF\" + f\"/PR{PR}/p_half/PR{PR}_500_20000day_6hourly_p_half.dat\"\n",
    "\n",
    "#     print(f\"PR={PR}, ---1---\")\n",
    "#     with h5py.File(qv_diff_file_path, 'r') as file:\n",
    "#         qv_diff = np.asarray(file['qv_diff'][:, :, :, :])\n",
    "        \n",
    "#     print(f\"PR={PR}, ---2---\")\n",
    "#     with h5py.File(p_half_file_path, 'r') as file:\n",
    "#         p_half = np.asarray(file['p_half'][:, :, :, :])\n",
    "\n",
    "#     print(f\"PR={PR}, ---3---\")\n",
    "#     prec = calculate_prec(qv_diff, p_half)\n",
    "#     save_to_hdf5(prec, \"prec\", common_path + f\"/PR{PR}/prec/PR{PR}_500_20000day_6hourly_prec_0_300hPa_sum.dat\") # (time, y, x)\n",
    "#     gc.collect()\n",
    "#     print(f\"PR={PR}, ---4---\")\n",
    "    \n",
    "# def process_pr_with_chunk(PR, chunk_size=1000):\n",
    "#     print(f\"Processing PR={PR}\")\n",
    "#     qv_diff_file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/qv_diff/PR{PR}_500_20000day_6hourly_qv_diff.dat\"\n",
    "#     p_half_file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/p_half/PR{PR}_500_20000day_6hourly_p_half.dat\"\n",
    "\n",
    "#     print(f\"PR={PR}, ---1---\")\n",
    "#     with h5py.File(qv_diff_file_path, 'r') as qv_diff_file, h5py.File(p_half_file_path, 'r') as p_half_file:\n",
    "#         qv_diff = qv_diff_file['qv_diff']\n",
    "#         p_half = p_half_file['p_half']\n",
    "        \n",
    "#         time_len = qv_diff.shape[0]  # Assuming time is the first dimension\n",
    "#         prec_total = np.zeros((time_len, qv_diff.shape[2], qv_diff.shape[3]))  # For storing total precipitation\n",
    "        \n",
    "#         for start in range(0, time_len, chunk_size):\n",
    "#             end = min(start + chunk_size, time_len)\n",
    "#             print(f\"Processing chunk {start} to {end} for PR={PR}\")\n",
    "            \n",
    "#             # Load the chunk\n",
    "#             qv_diff_chunk = qv_diff[start:end, :, :, :]\n",
    "#             p_half_chunk = p_half[start:end, :, :, :]\n",
    "            \n",
    "#             # Process the chunk\n",
    "#             prec_chunk = calculate_prec_chunk(qv_diff_chunk, p_half_chunk)\n",
    "#             prec_total[start:end, :, :] = prec_chunk\n",
    "            \n",
    "#             # Free memory\n",
    "#             del qv_diff_chunk, p_half_chunk, prec_chunk\n",
    "#             gc.collect()\n",
    "\n",
    "#     save_to_hdf5(prec_total, \"prec\", f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/prec/PR{PR}_500_20000day_6hourly_prec_0_300hPa_sum.dat\")\n",
    "#     print(f\"PR={PR}, ---4---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bcf812-3e9f-4c48-8fe7-5d3993fb3750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PR=10Processing PR=40Processing PR=20Processing PR=50Processing PR=30Processing PR=0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PR=10, ---1---PR=40, ---1---PR=30, ---1---PR=20, ---1---PR=50, ---1---PR=0, ---1---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def save_to_hdf5(data, data_name, file_path):\n",
    "#     with h5py.File(file_path, 'w') as file:\n",
    "#         file.create_dataset(data_name, data=data)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     PR_values = [0, 10, 20, 30, 40, 50]\n",
    "#     chunk_size = 1000  # You can adjust this based on memory availability\n",
    "#     with Pool(processes=6) as pool:\n",
    "#         pool.starmap(process_pr_with_chunk, [(PR, chunk_size) for PR in PR_values])\n",
    "#     print(\"All processes completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea90e99-307f-4d55-9069-bd26d6c6b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import h5py\n",
    "# import gc\n",
    "# import time\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "# def calculate_prec(qv_diff, p_half):\n",
    "#     \"\"\"Calculate precipitation from specific humidity difference and half-level pressures.\"\"\"\n",
    "#     g = 9.81\n",
    "#     Prec = np.zeros(qv_diff.shape)\n",
    "#     for i in range(1, 20 - 1):\n",
    "#         Prec[:, i, :, :] = 1 / g * qv_diff[:, i, :, :] * (p_half[:, i + 1, :, :] - p_half[:, i, :, :])\n",
    "#     Prec[:, 0, :, :] = 1 / g * qv_diff[:, 0, :, :] * (p_half[:, 1, :, :] - p_half[:, 0, :, :])\n",
    "#     Prec_mean = np.nansum(Prec, axis=(1))  # (time, y, x)\n",
    "#     return Prec_mean\n",
    "\n",
    "# def process_pr(PR):\n",
    "#     \"\"\"Process data for a specific PR value.\"\"\"\n",
    "#     print(f\"Processing PR={PR}\")\n",
    "#     common_path = \"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF\"\n",
    "#     qv_diff_file_path = f\"{common_path}/PR{PR}/qv_diff/PR{PR}_500_20000day_6hourly_qv_diff.dat\"\n",
    "#     p_half_file_path = f\"{common_path}/PR{PR}/p_half/PR{PR}_500_20000day_6hourly_p_half.dat\"\n",
    "#     prec_file_path = f\"{common_path}/PR{PR}/prec/PR{PR}_500_20000day_6hourly_prec.dat\"\n",
    "\n",
    "#     # Open HDF5 files once and process data in chunks\n",
    "#     with h5py.File(qv_diff_file_path, 'r') as qv_diff_file, h5py.File(p_half_file_path, 'r') as p_half_file:\n",
    "#         qv_diff_dataset = qv_diff_file['qv_diff']\n",
    "#         p_half_dataset = p_half_file['p_half']\n",
    "#         num_time_steps = qv_diff_dataset.shape[0]\n",
    "#         chunk_size = 100  # Define chunk size to optimize memory usage\n",
    "\n",
    "#         # Initialize array to store the results\n",
    "#         prec_data = np.zeros((num_time_steps, 64, 128), dtype='f4')\n",
    "\n",
    "#         # Process data in chunks\n",
    "#         for start_idx in range(0, num_time_steps, chunk_size):\n",
    "#             end_idx = min(start_idx + chunk_size, num_time_steps)\n",
    "\n",
    "#             qv_diff_chunk = qv_diff_dataset[start_idx:end_idx, :, :, :]\n",
    "#             p_half_chunk = p_half_dataset[start_idx:end_idx, :, :, :]\n",
    "\n",
    "#             # Calculate precipitation for the current chunk\n",
    "#             prec_chunk = calculate_prec(qv_diff_chunk, p_half_chunk)\n",
    "\n",
    "#             # Save the processed chunk to the array\n",
    "#             prec_data[start_idx:end_idx, :, :] = prec_chunk\n",
    "\n",
    "#             # Clear memory\n",
    "#             del qv_diff_chunk, p_half_chunk, prec_chunk\n",
    "#             gc.collect()\n",
    "\n",
    "#         # Save the entire dataset to an HDF5 file using your defined function\n",
    "#         save_to_hdf5(prec_data, \"prec\", prec_file_path)\n",
    "\n",
    "#     print(f\"Completed processing for PR={PR}\")\n",
    "\n",
    "# def save_to_hdf5(data, data_name, file_path):\n",
    "#     \"\"\"Save data to an HDF5 file.\"\"\"\n",
    "#     with h5py.File(file_path, 'w') as file:\n",
    "#         file.create_dataset(data_name, data=data)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     PR_values = [0, 10, 20, 30, 40, 50]\n",
    "#     start_time = time.time()\n",
    "#     with Pool(processes=6) as pool:\n",
    "#         pool.map(process_pr, PR_values)\n",
    "#     print(\"All processes completed\")\n",
    "#     print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe6dec0-afc3-409f-8d60-6faf4292b0dd",
   "metadata": {},
   "source": [
    "# EP flux = - u'v' j + ${f_0 \\over N^2} $\\partial$ $\\theta$ \\over $\\partial z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e836bda-b4ce-420b-8cd3-31a303f5bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PR = 30 => startfrom 20000 day == final, it doesn't exit xyzt, which is missing\n",
    "# # Constants\n",
    "# # PR = 30\n",
    "\n",
    "# # Saving b and v'b'\n",
    "# for PR in [0,10,30,50]:\n",
    "#     print(\"PR=\", PR)\n",
    "#     days = range(500, 20000, 25)\n",
    "#     num_days = len(days)\n",
    "#     num_time_steps = 100  # Assuming each file contributes 100 time steps\n",
    "#     z_dim, y_dim, x_dim = 20, 64, 128  # Dimensions of z, y, x\n",
    "    \n",
    "#     # Pre-allocate the entire array\n",
    "#     # u_prime_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "#     v_prime_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "#     theta_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "#     theta_prime_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "#     b_prime_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "    \n",
    "#     # Initialize Zonal_Calculation object once if all files have the same structure\n",
    "#     t_zonal_calc  = Zonal_Calculation(3, \"grid_t_c_xyzt\")\n",
    "#     p_zonal_calc  = Zonal_Calculation(3, \"grid_p_full_xyzt\")\n",
    "#     ps_zonal_calc = Zonal_Calculation(3, \"grid_ps_c_xyzt\")\n",
    "#     print(\"---1---\")\n",
    "    \n",
    "#     u_zonal_calc  = Zonal_Calculation(3, \"grid_u_c_xyzt\")\n",
    "#     v_zonal_calc  = Zonal_Calculation(3, \"grid_v_c_xyzt\")\n",
    "    \n",
    "    \n",
    "#     Rd = 287\n",
    "#     cp = 1004\n",
    "#     g  = 9.81\n",
    "#     # Process each day\n",
    "#     for i, start in enumerate(days):\n",
    "#         file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/HSt42_{PR}_6hourly_CC_OK/RH80_PR{PR}_20000day_startfrom_{start}day_final.dat\"    \n",
    "#         t_data  = t_zonal_calc.load_data(file_path)\n",
    "#         p_data  = p_zonal_calc.load_data(file_path)\n",
    "#         ps_data = ps_zonal_calc.load_data(file_path)\n",
    "#         # u_data = u_zonal_calc.load_data(file_path)\n",
    "#         v_data = v_zonal_calc.load_data(file_path)\n",
    "\n",
    "#         # Cal prime, theta_prime can't be used by class because Dycore dosen't output grid_theta_xyzt\n",
    "#         # u_prime     = u_zonal_calc.zonal_anomaly(u_data)   # FIXED!!!!!! 8/4 u_prime     = v_zonal_calc.zonal_anomaly(v_data)\n",
    "#         v_prime     = v_zonal_calc.zonal_anomaly(v_data)  \n",
    "        \n",
    "#         theta       = t_data * (ps_data / p_data)**(Rd/cp)\n",
    "#         theta_prime = theta - theta.mean(axis=3)[:,:,:,np.newaxis] # FIXED!!! Origin:theta_prime = t_zonal_calc.zonal_anomaly(theta) # \n",
    "        \n",
    "#         # Insert data into the pre-allocated array\n",
    "#         # u_prime_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = u_prime\n",
    "#         v_prime_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = v_prime\n",
    "#         theta_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :]   = theta\n",
    "#         theta_prime_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :]   = theta_prime\n",
    "        \n",
    "#         # Optional progress logging\n",
    "#         if start % 1000 == 0:\n",
    "#             print(f\"Processed day {start}\")\n",
    "    \n",
    "#     print(\"done\")\n",
    "#     del t_data, ps_data, p_data\n",
    "\n",
    "#     # Saving\n",
    "#     # u_prime\n",
    "#     # u_prime_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/u_prime/PR{PR}_500_20000day_6hourly_u_prime.dat\"  \n",
    "#     # with h5py.File(u_prime_file, 'w') as u_prime_file0:\n",
    "#     #     u_prime_file0.create_dataset('u_prime',data=u_prime_final)\n",
    "#     # del u_prime_final\n",
    "#     # print(\"---7---\")\n",
    "    \n",
    "#     # # v_prime\n",
    "#     # v_prime_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/v_prime/PR{PR}_500_20000day_6hourly_v_prime.dat\"  \n",
    "#     # with h5py.File(v_prime_file, 'w') as v_prime_file0:\n",
    "#     #     v_prime_file0.create_dataset('v_prime',data=v_prime_final)\n",
    "#     # del v_prime_final\n",
    "#     # print(\"---8---\")\n",
    "\n",
    "#     # # theta\n",
    "#     # theta_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/theta/PR{PR}_500_20000day_6hourly_theta.dat\"  \n",
    "#     # with h5py.File(theta_file, 'w') as theta_file0:\n",
    "#     #     theta_file0.create_dataset('theta',data=theta_final)\n",
    "#     # del theta_final\n",
    "#     # print(\"---9---\")\n",
    "    \n",
    "#     # # theta_prime\n",
    "#     # theta_prime_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/theta_prime/PR{PR}_500_20000day_6hourly_theta_prime.dat\"  \n",
    "#     # with h5py.File(theta_prime_file, 'w') as theta_prime_file0:\n",
    "#     #     theta_prime_file0.create_dataset('theta_prime',data=theta_prime_final)\n",
    "#     # del theta_prime_final\n",
    "#     # print(\"---10---\")\n",
    "\n",
    "#     # b_prime\n",
    "#     b_prime = theta_prime_final / np.mean(theta_final, axis=3)[:,:,:,np.newaxis] * 9.81\n",
    "#     b_prime_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/b_prime/PR{PR}_500_20000day_6hourly_b_prime.dat\"\n",
    "#     with h5py.File(b_prime_file, 'w') as file:\n",
    "#         file.create_dataset('b_prime', data=b_prime)\n",
    "#     del b_prime\n",
    "\n",
    "#     # Read long time mean theta to calculate b_prime\n",
    "#     # theta_long_time_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/theta/PR{PR}_500_20000day_6hourly_theta.dat\"\n",
    "#     # theta_long_time_file_read = h5py.File(theta_long_time_file, \"r\")\n",
    "#     # theta_long_time = np.asarray(theta_long_time_file_read['theta'][:,:,:,:])\n",
    "#     # print(\"---5---\")\n",
    "\n",
    "#     # Read long time mean theta to calculate b_prime\n",
    "#     # v_prime_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/v_prime/PR{PR}_500_20000day_6hourly_v_prime.dat\"\n",
    "#     # v_prime_file_read = h5py.File(v_prime_file, \"r\")\n",
    "#     # v_prime = np.asarray(v_prime_file_read['v_prime'][:5000*4,:,:,:])\n",
    "#     # print(\"---5---\")\n",
    "    \n",
    "#     # v_prime_b_prime\n",
    "#     vb_mean = np.mean(v_prime * b_prime, axis=(0,3))\n",
    "#     del theta_prime_final, theta_final\n",
    "#     print(\"---6---\")\n",
    "\n",
    "#     # Saving\n",
    "#     vb_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/v_prime_b_prime/PR{PR}_500_20000day_6hourly_v_prime_b_prime.dat\"  \n",
    "#     with h5py.File(vb_file, 'w') as vb_file0:\n",
    "#         vb_file0.create_dataset('v_prime_b_prime',data=vb_mean)\n",
    "#     del vb_mean\n",
    "#     # print(\"---5---\")\n",
    "#     Original code\n",
    "# def process_pr(PR):\n",
    "#     print(\"Processing PR=\", PR)\n",
    "#     start = time.time()\n",
    "#     days = range(500, 20000, 25)\n",
    "#     num_days = len(days)\n",
    "#     num_time_steps = 100  # Assuming each file contributes 100 time steps\n",
    "#     z_dim, y_dim, x_dim = 20, 64, 128  # Dimensions of z, y, x\n",
    "\n",
    "#     # t_zonal_calc = Zonal_Calculation(3, \"grid_t_c_xyzt\")\n",
    "#     p_zonal_calc = Zonal_Calculation(3, \"grid_p_full_xyzt\")\n",
    "#     ps_zonal_calc = Zonal_Calculation(3, \"grid_ps_c_xyzt\")\n",
    "#     # v_zonal_calc = Zonal_Calculation(3, \"grid_v_c_xyzt\")\n",
    "#     # u_zonal_calc = Zonal_Calculation(3, \"grid_u_c_xyzt\")\n",
    "#     print(\"---Initialized Zonal Calculators---\")\n",
    "\n",
    "#     # Memory-mapped arrays for large datasets\n",
    "#     # u_prime_final = np.memmap('/tmp/u_prime_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     # v_prime_final = np.memmap('/tmp/v_prime_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     # theta_final = np.memmap('/tmp/theta_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     # theta_prime_final = np.memmap('/tmp/theta_prime_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     # 08/14 add output pressure, surface pressure, and sigma\n",
    "#     p_final = np.memmap('/tmp/p_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     ps_final = np.memmap('/tmp/ps_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     sigma_final = np.memmap('/tmp/sigma_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     # add u_final\n",
    "#     # u_final = np.memmap('/tmp/u_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "\n",
    "#     Rd, cp, g = 287, 1004, 9.81  # Constants\n",
    "\n",
    "#     # Process each day in chunks\n",
    "#     chunk_size = 10  # Adjust chunk size as needed\n",
    "#     for i, start in enumerate(days):\n",
    "#         file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/HSt42_{PR}_6hourly_CC_OK/RH80_PR{PR}_20000day_startfrom_{start}day_final.dat\"\n",
    "        \n",
    "#         # Process data in smaller chunks to reduce memory usage\n",
    "#         for chunk_start in range(0, num_time_steps, chunk_size):\n",
    "#             chunk_end = min(chunk_start + chunk_size, num_time_steps)\n",
    "\n",
    "#             # t_data = t_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "#             p_data = p_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "#             ps_data = ps_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "#             # v_data = v_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "#             # u_data = u_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "\n",
    "#             # u_prime = u_zonal_calc.zonal_anomaly(u_data)\n",
    "#             # v_prime = v_zonal_calc.zonal_anomaly(v_data)\n",
    "#             # theta = t_data * (ps_data / p_data)**(Rd/cp)\n",
    "#             # theta_prime = theta - np.mean(theta, axis=3, keepdims=True)\n",
    "\n",
    "#             # u_prime_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = u_prime\n",
    "#             # v_prime_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = v_prime\n",
    "#             # theta_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = theta\n",
    "#             # theta_prime_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = theta_prime\n",
    "#             # 08/14 add output pressure, surface pressure, and sigma\n",
    "#             p_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = p_data\n",
    "#             ps_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = ps_data\n",
    "\n",
    "#             # u_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = u_data\n",
    "\n",
    "#             # Clear memory\n",
    "#             # del u_prime, v_prime, theta, theta_prime\n",
    "#             # del u_data\n",
    "            \n",
    "#             # 08/14 add\n",
    "#             del p_data, ps_data\n",
    "#             gc.collect()\n",
    "            \n",
    "#         if start % 1000 == 0:\n",
    "#             print(f\"Processed day {start}\")\n",
    "#         gc.collect()\n",
    "        \n",
    "\n",
    "#     # Compute and save b_prime\n",
    "#     # theta_final_zonal_mean = np.mean(theta_final, axis=3, keepdims=True)\n",
    "#     # theta_final_add_dims = np.empty_like(theta_final)\n",
    "#     # theta_final_add_dims[:] = theta_final_zonal_mean\n",
    "\n",
    "#     # b_prime = theta_prime_final / theta_final_add_dims * g\n",
    "#     # Save data using memory-mapped arrays\n",
    "#     common_path = \"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF\"\n",
    "#     # save_to_hdf5(u_prime_final, \"u_prime\", common_path + f\"/PR{PR}/u_prime/PR{PR}_500_20000day_6hourly_u_prime.dat\")\n",
    "#     # save_to_hdf5(v_prime_final, \"v_prime\", common_path + f\"/PR{PR}/v_prime/PR{PR}_500_20000day_6hourly_v_prime.dat\")\n",
    "#     # save_to_hdf5(u_prime_final * v_prime_final, \"EMF\", common_path + f\"/PR{PR}/EMF/PR{PR}_500_20000day_6hourly_EMF.dat\")\n",
    "#     # save_to_hdf5((u_prime_final * v_prime_final).mean(axis=3), \"EMF\", common_path + f\"/PR{PR}/EMF/PR{PR}_500_20000day_6hourly_EMF_zonal_mean.dat\")\n",
    "#     # save_to_hdf5(theta_final, \"theta\", common_path + f\"/PR{PR}/theta/PR{PR}_500_20000day_6hourly_theta.dat\")\n",
    "#     # save_to_hdf5(theta_prime_final, \"theta_prime\", common_path + f\"/PR{PR}/theta_prime/PR{PR}_500_20000day_6hourly_theta_prime.dat\")\n",
    "\n",
    "#     # # Saving zonal mean var\n",
    "#     # save_to_hdf5(theta_final.mean(axis=3), \"theta\", common_path + f\"/PR{PR}/theta/PR{PR}_500_20000day_6hourly_theta_zonal_mean.dat\")\n",
    "    \n",
    "#     # # Save b_prime and its zonal mean\n",
    "#     # save_to_hdf5(b_prime, \"b_prime\", common_path + f\"/PR{PR}/b_prime/PR{PR}_500_20000day_6hourly_b_prime.dat\")\n",
    "#     # save_to_hdf5(b_prime.mean(axis=3), \"b_prime\", common_path + f\"/PR{PR}/b_prime/PR{PR}_500_20000day_6hourly_b_prime_zonal_mean.dat\")\n",
    "\n",
    "#     # save_to_hdf5(v_prime_final * b_prime, \"v_prime_b_prime\", common_path + f\"/PR{PR}/v_prime_b_prime/PR{PR}_500_20000day_6hourly_v_prime_b_prime.dat\")\n",
    "#     # save_to_hdf5((v_prime_final * b_prime).mean(axis=3), \"v_prime_b_prime\", common_path + f\"/PR{PR}/v_prime_b_prime/PR{PR}_500_20000day_6hourly_v_prime_b_prime_zonal_mean.dat\")\n",
    "\n",
    "#     # 08/14 Saving p, ps, and sigma\n",
    "#     sigma = p_final / ps_final\n",
    "#     save_to_hdf5(p_final, \"p\", common_path + f\"/PR{PR}/p/PR{PR}_500_20000day_6hourly_p.dat\")\n",
    "#     save_to_hdf5(ps_final, \"ps\", common_path + f\"/PR{PR}/ps/PR{PR}_500_20000day_6hourly_ps.dat\")\n",
    "#     save_to_hdf5(sigma, \"sigma\", common_path + f\"/PR{PR}/sigma/PR{PR}_500_20000day_6hourly_sigma.dat\")\n",
    "#     save_to_hdf5(sigma.mean(axis=(0,2,3)), \"sigma\", common_path + f\"/PR{PR}/sigma/PR{PR}_500_20000day_6hourly_sigma_only_z.dat\")\n",
    "#     # 08/14 add u\n",
    "#     # save_to_hdf5(u_final, \"u\", common_path + f\"/PR{PR}/u/PR{PR}_500_20000day_6hourly_u.dat\")\n",
    "#     # save_to_hdf5(u_final.mean(axis=(1,3)), \"u\", common_path + f\"/PR{PR}/u/PR{PR}_500_20000day_6hourly_u_time_y.dat\")\n",
    "\n",
    "    \n",
    "    \n",
    "#     # Clean up memory-mapped arrays\n",
    "#     # del u_prime_final, v_prime_final, theta_final, theta_prime_final, b_prime\n",
    "#     # 08/14 add\n",
    "#     del p_final, ps_final, sigma\n",
    "#     # del u_final\n",
    "#     gc.collect()\n",
    "    \n",
    "#     end = time.time()\n",
    "#     print(\"執行時間：%f 秒\" % (end - start))\n",
    "#     print(f\"Completed processing and saved b_prime for PR={PR}\")\n",
    "\n",
    "# def save_to_hdf5(data, data_name, file_path):\n",
    "#     with h5py.File(file_path, 'w') as file:\n",
    "#         file.create_dataset(data_name, data=data)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     PR_values = [20]\n",
    "#     process_pr(PR_values[0])\n",
    "#     print(\"All processes completed\")\n",
    "\n",
    "# print(\"Basic variables saving done!\")\n",
    "# ###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a059a3-ebbb-4a09-b086-1ac5c5917c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PR=0Processing PR=10Processing PR=40Processing PR=20Processing PR=30Processing PR=50\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---Initialized Zonal Calculators------Initialized Zonal Calculators------Initialized Zonal Calculators------Initialized Zonal Calculators------Initialized Zonal Calculators------Initialized Zonal Calculators---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m     PR_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m50\u001b[39m]\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 92\u001b[0m         \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_pr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPR_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll processes completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBasic variables saving done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML22/lib/python3.12/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML22/lib/python3.12/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML22/lib/python3.12/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML22/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/ML22/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import h5py\n",
    "# import os\n",
    "# import gc\n",
    "# from multiprocessing import Pool\n",
    "# import time\n",
    "\n",
    "# def process_pr(PR):\n",
    "#     print(f\"Processing PR={PR}\")\n",
    "#     start_time = time.time()\n",
    "#     days = range(500, 20000, 25)\n",
    "#     num_days = len(days)\n",
    "#     num_time_steps = 100  # Assuming each file contributes 100 time steps\n",
    "#     z_dim, y_dim, x_dim = 20, 64, 128  # Dimensions of z, y, x\n",
    "\n",
    "#     # Initialize Zonal Calculators\n",
    "#     p_half_zonal_calc = Zonal_Calculation(3, \"grid_p_half_xyzt\")\n",
    "#     qv_diff_zonal_calc = Zonal_Calculation(3, \"grid_tracers_diff_xyzt\")\n",
    "#     print(\"---Initialized Zonal Calculators---\")\n",
    "\n",
    "#     # Memory-mapped arrays for the final datasets\n",
    "#     p_half_final = np.memmap('/data92/PeterChang/p_half_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim+1, y_dim, x_dim))\n",
    "#     qv_diff_final = np.memmap('/data92/PeterChang/qv_diff_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "    \n",
    "#     Rd, cp, g = 287, 1004, 9.81  # Constants\n",
    "\n",
    "#     # Process each day in chunks\n",
    "#     chunk_size = 10  # Adjust chunk size as needed\n",
    "#     for i, start_day in enumerate(days):\n",
    "#         file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/HSt42_{PR}_6hourly_CC_OK/RH80_PR{PR}_20000day_startfrom_{start_day}day_final.dat\"\n",
    "        \n",
    "#         # Process data in smaller chunks to reduce memory usage\n",
    "#         for chunk_start in range(0, num_time_steps, chunk_size):\n",
    "#             chunk_end = min(chunk_start + chunk_size, num_time_steps)\n",
    "\n",
    "#             # Load data\n",
    "#             p_half_data = p_half_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "#             qv_diff_data = qv_diff_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "\n",
    "#             # Save the data in the memory-mapped arrays\n",
    "#             p_half_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = p_half_data\n",
    "#             qv_diff_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = qv_diff_data\n",
    "\n",
    "#             # Clear memory\n",
    "#             del p_half_data, qv_diff_data\n",
    "#             gc.collect()\n",
    "            \n",
    "#         if (i + 1) % 5 == 0:\n",
    "#             print(f\"Processed up to day {start_day} for PR={PR}\")\n",
    "#         gc.collect()\n",
    "        \n",
    "#     # Save data to HDF5\n",
    "#     common_path = \"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF\"\n",
    "    \n",
    "#     save_to_hdf5(p_half_final, \"p_half\", common_path + f\"/PR{PR}/p_half/PR{PR}_500_20000day_6hourly_p_half.dat\")\n",
    "#     save_to_hdf5(p_half_final.mean(axis=(1,3)), \"p_half\", common_path + f\"/PR{PR}/p_half/PR{PR}_500_20000day_6hourly_p_half_time_y.dat\")\n",
    "#     save_to_hdf5(qv_diff_final, \"qv_diff\", common_path + f\"/PR{PR}/qv_diff/PR{PR}_500_20000day_6hourly_qv_diff.dat\")\n",
    "#     save_to_hdf5(qv_diff_final.mean(axis=(1,3)), \"qv_diff\", common_path + f\"/PR{PR}/qv_diff/PR{PR}_500_20000day_6hourly_qv_diff_time_y.dat\")\n",
    "\n",
    "#     # Calculate precipitation\n",
    "#     prec_final = calculate_prec(qv_diff_final, p_half_final)\n",
    "#     save_to_hdf5(prec_final, \"prec\", common_path + f\"/PR{PR}/prec/PR{PR}_500_20000day_6hourly_prec.dat\")\n",
    "\n",
    "#     # Clean up memory-mapped arrays\n",
    "#     del p_half_final, qv_diff_final, prec_final\n",
    "#     gc.collect()\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     print(f\"Execution time for PR={PR}: {end_time - start_time:.2f} seconds\")\n",
    "#     print(f\"Completed processing for PR={PR}\")\n",
    "\n",
    "# def save_to_hdf5(data, data_name, file_path):\n",
    "#     with h5py.File(file_path, 'w') as file:\n",
    "#         file.create_dataset(data_name, data=data)\n",
    "\n",
    "# def calculate_prec(qv_diff, p_half):\n",
    "#     \"\"\"Calculate precipitation from specific humidity difference and half-level pressures.\"\"\"\n",
    "#     g = 9.81\n",
    "#     Prec = np.zeros(qv_diff.shape)\n",
    "#     for i in range(1, 20 - 1):\n",
    "#         Prec[:, i, :, :] = 1 / g * qv_diff[:, i, :, :] * (p_half[:, i + 1, :, :] - p_half[:, i, :, :])\n",
    "#     Prec[:, 0, :, :] = 1 / g * qv_diff[:, 0, :, :] * (p_half[:, 1, :, :] - p_half[:, 0, :, :])\n",
    "#     Prec_mean = np.nansum(Prec, axis=(1))  # (time, y, x)\n",
    "#     return Prec_mean\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     PR_values = [0, 10, 20, 30, 40, 50]\n",
    "#     with Pool(processes=1) as pool:\n",
    "#         pool.map(process_pr, PR_values)\n",
    "#     print(\"All processes completed\")\n",
    "\n",
    "# print(\"Basic variables saving done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd24bb-4f4f-4295-bfb8-d6ad4072f859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using ChatGPT\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import gc\n",
    "# import time\n",
    "\n",
    "# def process_pr(PR):\n",
    "#     print(\"Processing PR=\", PR)\n",
    "#     start_time = time.time()\n",
    "#     days = range(500, 20000, 25)\n",
    "#     num_days = len(days)\n",
    "#     num_time_steps = 100  # Assuming each file contributes 100 time steps\n",
    "#     z_dim, y_dim, x_dim = 20, 64, 128  # Dimensions of z, y, x\n",
    "\n",
    "#     t_zonal_calc = Zonal_Calculation(3, \"grid_t_c_xyzt\")\n",
    "#     p_zonal_calc = Zonal_Calculation(3, \"grid_p_full_xyzt\")\n",
    "#     ps_zonal_calc = Zonal_Calculation(3, \"grid_ps_c_xyzt\")\n",
    "#     v_zonal_calc = Zonal_Calculation(3, \"grid_v_c_xyzt\")\n",
    "#     u_zonal_calc = Zonal_Calculation(3, \"grid_u_c_xyzt\")\n",
    "\n",
    "#     print(\"---Initialized Zonal Calculators---\")\n",
    "\n",
    "#     # Memory-mapped arrays for large datasets\n",
    "#     u_prime_final = np.memmap('/tmp/u_prime_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     v_prime_final = np.memmap('/tmp/v_prime_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     theta_final = np.memmap('/tmp/theta_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     theta_prime_final = np.memmap('/tmp/theta_prime_final.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "\n",
    "#     Rd, cp, g = 287, 1004, 9.81  # Constants\n",
    "\n",
    "#     # Process each day in chunks\n",
    "#     chunk_size = 1500  # Adjust chunk size as needed\n",
    "#     for i, start_day in enumerate(days):\n",
    "#         file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/HSt42_{PR}_6hourly_CC_OK/RH80_PR{PR}_20000day_startfrom_{start_day}day_final.dat\"\n",
    "        \n",
    "#         # Process data in smaller chunks to reduce memory usage\n",
    "#         for chunk_start in range(0, num_time_steps, chunk_size):\n",
    "#             chunk_end = min(chunk_start + chunk_size, num_time_steps)\n",
    "\n",
    "#             t_data = t_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "#             p_data = p_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "#             ps_data = ps_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "#             v_data = v_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "#             u_data = u_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "\n",
    "#             u_prime     = u_zonal_calc.zonal_anomaly(u_data)\n",
    "#             v_prime     = v_zonal_calc.zonal_anomaly(v_data)\n",
    "#             theta       = t_data * (ps_data / p_data) ** (Rd / cp)\n",
    "#             theta_prime = theta - theta.mean(axis=3, keepdims=True)\n",
    "\n",
    "#             u_prime_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = u_prime\n",
    "#             v_prime_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = v_prime\n",
    "#             theta_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = theta\n",
    "#             theta_prime_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = theta_prime\n",
    "\n",
    "#             # Clear memory\n",
    "#             del u_prime, v_prime, theta, theta_prime\n",
    "#             gc.collect()\n",
    "\n",
    "#         if i % 1000 == 0:\n",
    "#             print(f\"Processed up to day {start_day}\")\n",
    "#     gc.collect()\n",
    "\n",
    "#     # Compute and save b_prime\n",
    "#     theta_final_zonal_mean = np.mean(theta_final, axis=3, keepdims=True)\n",
    "#     b_prime = ne.evaluate('theta_prime_final / theta_final_zonal_mean * g')\n",
    "\n",
    "#     # Save data using memory-mapped arrays\n",
    "#     common_path = \"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF\"\n",
    "#     save_to_hdf5(u_prime_final, \"u_prime\", common_path + f\"/PR{PR}/u_prime/PR{PR}_500_20000day_6hourly_u_prime.dat\")\n",
    "#     save_to_hdf5(v_prime_final, \"v_prime\", common_path + f\"/PR{PR}/v_prime/PR{PR}_500_20000day_6hourly_v_prime.dat\")\n",
    "#     save_to_hdf5(theta_final, \"theta\", common_path + f\"/PR{PR}/theta/PR{PR}_500_20000day_6hourly_theta.dat\")\n",
    "#     save_to_hdf5(theta_prime_final, \"theta_prime\", common_path + f\"/PR{PR}/theta_prime/PR{PR}_500_20000day_6hourly_theta_prime.dat\")\n",
    "\n",
    "#     # Save zonal mean variables\n",
    "#     save_to_hdf5(theta_final.mean(axis=3), \"theta_zonal_mean\", common_path + f\"/PR{PR}/theta/PR{PR}_500_20000day_6hourly_theta_zonal_mean.dat\")\n",
    "#     save_to_hdf5(b_prime, \"b_prime\", common_path + f\"/PR{PR}/b_prime/PR{PR}_500_20000day_6hourly_b_prime.dat\")\n",
    "#     save_to_hdf5(b_prime.mean(axis=3), \"b_prime_zonal_mean\", common_path + f\"/PR{PR}/b_prime/PR{PR}_500_20000day_6hourly_b_prime_zonal_mean.dat\")\n",
    "\n",
    "#     # Clean up memory-mapped arrays\n",
    "#     del u_prime_final, v_prime_final, theta_final, theta_prime_final, b_prime\n",
    "#     gc.collect()\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     print(f\"Completed processing and saving for PR={PR} in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# def save_to_hdf5(data, data_name, file_path):\n",
    "#     with h5py.File(file_path, 'w') as file:\n",
    "#         file.create_dataset(data_name, data=data)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     PR_values = [40]\n",
    "#     process_pr(PR_values[0])\n",
    "#     print(\"All processes completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f98b112-87c6-4b1b-a9a5-b6c33d685c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving z\n",
    "# def process_pr(PR):\n",
    "#     print(f\"PR={PR}\")\n",
    "#     days = range(500, 20000, 25)\n",
    "#     num_days = len(days)\n",
    "#     num_time_steps = 100  # Assuming each file contributes 100 time steps\n",
    "#     z_dim, y_dim, x_dim = 20, 64, 128  # Dimensions of z, y, x\n",
    "\n",
    "#     # Initialize Zonal_Calculation object once if all files have the same structure\n",
    "#     z_zonal_calc = Zonal_Calculation(3, \"grid_z_full_xyzt\")\n",
    "#     u_zonal_calc = Zonal_Calculation(3, \"grid_u_c_xyzt\")\n",
    "#     v_zonal_calc = Zonal_Calculation(3, \"grid_v_c_xyzt\")\n",
    "#     t_zonal_calc = Zonal_Calculation(3, \"grid_t_c_xyzt\")\n",
    "    \n",
    "    \n",
    "#     # div_zonal_calc = Zonal_Calculation(3, \"grid_div_xyzt\")\n",
    "    \n",
    "#     # p_zonal_calc = Zonal_Calculation(3, \"grid_p_full_xyzt\")\n",
    "#     # ps_zonal_calc = Zonal_Calculation(3, \"grid_ps_c_xyzt\")\n",
    "\n",
    "#     # Pre-allocate the entire array\n",
    "#     # b_prime_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "#     z_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "#     u_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "#     v_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "#     t_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "    \n",
    "\n",
    "#     # div_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "    \n",
    "#     # theta_prime_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "\n",
    "#     Rd = 287\n",
    "#     cp = 1004\n",
    "\n",
    "#     # Process each day\n",
    "#     for i, start in enumerate(days):\n",
    "#         file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/HSt42_{PR}_6hourly_CC_OK/RH80_PR{PR}_20000day_startfrom_{start}day_final.dat\"\n",
    "#         z_data = z_zonal_calc.load_data(file_path)\n",
    "#         u_data = u_zonal_calc.load_data(file_path)\n",
    "#         v_data = v_zonal_calc.load_data(file_path)\n",
    "#         t_data = t_zonal_calc.load_data(file_path)\n",
    "\n",
    "#         # div_data = div_zonal_calc.load_data(file_path)\n",
    "        \n",
    "#         # p_data = p_zonal_calc.load_data(file_path)\n",
    "#         # ps_data = ps_zonal_calc.load_data(file_path)\n",
    "\n",
    "#         # theta = t_data * (ps_data / p_data) ** (Rd / cp)\n",
    "#         # theta_prime = theta - theta.mean(axis=3)[:, :, :, np.newaxis]\n",
    "#         z_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = z_data\n",
    "#         u_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = u_data\n",
    "#         v_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = v_data\n",
    "#         t_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = t_data\n",
    "        \n",
    "#         # div_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = div_data\n",
    "        \n",
    "#         # theta_prime_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = theta_prime\n",
    "\n",
    "#         if start % 1000 == 0:\n",
    "#             print(f\"Processed day {start}\")\n",
    "\n",
    "#     # b_prime = theta_prime_final / np.mean(theta_final, axis=0) * 9.81\n",
    "#     # # Save results\n",
    "#     # b_prime_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/b_prime/PR{PR}_500_20000day_6hourly_b_prime.dat\"\n",
    "#     # with h5py.File(b_prime_file, 'w') as file:\n",
    "#     #     file.create_dataset('b_prime', data=b_prime)\n",
    "\n",
    "#     # Save results\n",
    "#     z_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/z/PR{PR}_500_20000day_6hourly_z.dat\"\n",
    "#     with h5py.File(z_file, 'w') as file:\n",
    "#         file.create_dataset('z', data=z_final)\n",
    "\n",
    "#     # Save results\n",
    "#     u_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/u/PR{PR}_500_20000day_6hourly_u.dat\"\n",
    "#     with h5py.File(u_file, 'w') as file:\n",
    "#         file.create_dataset('u', data=u_final)\n",
    "\n",
    "#     # Save results\n",
    "#     v_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/v/PR{PR}_500_20000day_6hourly_v.dat\"\n",
    "#     with h5py.File(v_file, 'w') as file:\n",
    "#         file.create_dataset('v', data=v_final)\n",
    "\n",
    "#     # Save results\n",
    "#     t_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/t/PR{PR}_500_20000day_6hourly_t.dat\"\n",
    "#     with h5py.File(t_file, 'w') as file:\n",
    "#         file.create_dataset('t', data=t_final)\n",
    "#     print(f\"Completed PR={PR}\")\n",
    "#     return f\"Completed PR={PR}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e3504-d45e-4596-aab7-881abfabd592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PR=20Processing PR=0Processing PR=10Processing PR=30Processing PR=50Processing PR=40\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processed day 1000 for PR=50\n",
      "Processed day 1000 for PR=10\n",
      "Processed day 1000 for PR=30\n",
      "Processed day 1000 for PR=20\n",
      "Processed day 1000 for PR=40\n",
      "Processed day 1000 for PR=0\n",
      "Processed day 2000 for PR=30\n",
      "Processed day 2000 for PR=10\n",
      "Processed day 2000 for PR=0\n",
      "Processed day 2000 for PR=40\n",
      "Processed day 2000 for PR=50\n",
      "Processed day 2000 for PR=20\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import gc\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def process_pr(PR):\n",
    "    print(f\"Processing PR={PR}\")\n",
    "    \n",
    "    days = range(500, 20000, 25)\n",
    "    num_days = len(days)\n",
    "    num_time_steps = 100  # Assuming each file contributes 100 time steps\n",
    "    z_dim, y_dim, x_dim = 20, 64, 128  # Dimensions of z, y, x\n",
    "    \n",
    "    # Initialize Zonal_Calculation object once if all files have the same structure\n",
    "    # z_zonal_calc = Zonal_Calculation(3, \"grid_z_full_xyzt\")\n",
    "    u_zonal_calc = Zonal_Calculation(3, \"grid_u_c_xyzt\")\n",
    "    # v_zonal_calc = Zonal_Calculation(3, \"grid_v_c_xyzt\")\n",
    "    # t_zonal_calc = Zonal_Calculation(3, \"grid_t_c_xyzt\")\n",
    "\n",
    "    # Pre-allocate memory for results\n",
    "    z_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "    u_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "    v_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "    t_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "\n",
    "    # Load and process data for each day\n",
    "    for i, start in enumerate(days):\n",
    "        file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/HSt42_{PR}_6hourly_CC_OK/RH80_PR{PR}_20000day_startfrom_{start}day_final.dat\"\n",
    "        \n",
    "        # Load data using Zonal_Calculation\n",
    "        # z_data = z_zonal_calc.load_data(file_path)\n",
    "        u_data = u_zonal_calc.load_data(file_path)\n",
    "        # v_data = v_zonal_calc.load_data(file_path)\n",
    "        # t_data = t_zonal_calc.load_data(file_path)\n",
    "\n",
    "        # Save into pre-allocated arrays\n",
    "        slice_start = i * num_time_steps\n",
    "        slice_end = (i + 1) * num_time_steps\n",
    "        # z_final[slice_start:slice_end, :, :, :] = z_data\n",
    "        u_final[slice_start:slice_end, :, :, :] = u_data\n",
    "        # v_final[slice_start:slice_end, :, :, :] = v_data\n",
    "        # t_final[slice_start:slice_end, :, :, :] = t_data\n",
    "        \n",
    "        # Free up memory for large variables and run garbage collection\n",
    "        # del z_data, u_data, v_data, t_data  # Delete the temporary variables\n",
    "        del u_data  # Delete the temporary variables\n",
    "        \n",
    "        gc.collect()  # Explicitly call garbage collection to free memory\n",
    "        \n",
    "        if start % 1000 == 0:\n",
    "            print(f\"Processed day {start} for PR={PR}\")\n",
    "\n",
    "    # Save results to HDF5\n",
    "    output_paths = {\n",
    "        # 'z': f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/z/PR{PR}_500_20000day_6hourly_z.dat\",\n",
    "        'u': f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/u/PR{PR}_500_20000day_6hourly_u.dat\",\n",
    "        # 'v': f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/v/PR{PR}_500_20000day_6hourly_v.dat\",\n",
    "        # 't': f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/t/PR{PR}_500_20000day_6hourly_t.dat\"\n",
    "    }\n",
    "\n",
    "    for var_name, output_file in output_paths.items():\n",
    "        with h5py.File(output_file, 'w') as file:\n",
    "            file.create_dataset(var_name, data=locals()[f\"{var_name}_final\"])\n",
    "\n",
    "    print(f\"Completed processing PR={PR}\")\n",
    "    return f\"Completed PR={PR}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PR_values = [0, 10, 20, 30, 40, 50]\n",
    "    \n",
    "    # Adjust the number of processes based on system capabilities\n",
    "    with Pool(processes=6) as pool:  # Use 6 CPUs\n",
    "        results = pool.map(process_pr, PR_values)\n",
    "    \n",
    "    print(\"All processes completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6522ff-d333-4aa0-9074-37c8f44edaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving p\n",
    "# def process_pr(PR):\n",
    "#     print(f\"PR={PR}\")\n",
    "#     days = range(500, 20000, 25)\n",
    "#     num_days = len(days)\n",
    "#     num_time_steps = 100  # Assuming each file contributes 100 time steps\n",
    "#     z_dim, y_dim, x_dim = 20, 64, 128  # Dimensions of z, y, x\n",
    "\n",
    "#     # Initialize Zonal_Calculation object once if all files have the same structure\n",
    "#     # z_zonal_calc = Zonal_Calculation(3, \"grid_z_full_xyzt\")\n",
    "#     # div_zonal_calc = Zonal_Calculation(3, \"grid_div_xyzt\")\n",
    "    \n",
    "#     # p_zonal_calc = Zonal_Calculation(3, \"grid_p_full_xyzt\")\n",
    "#     p_zonal_calc = Zonal_Calculation(3, \"grid_tracers_c_xyzt\")\n",
    "    \n",
    "#     # ps_zonal_calc = Zonal_Calculation(3, \"grid_ps_c_xyzt\")\n",
    "\n",
    "#     # Pre-allocate the entire array\n",
    "#     # b_prime_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "#     # z_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "#     p_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "#     # div_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "    \n",
    "#     # theta_prime_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "\n",
    "#     Rd = 287\n",
    "#     cp = 1004\n",
    "\n",
    "#     # Process each day\n",
    "#     for i, start in enumerate(days):\n",
    "#         file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/HSt42_{PR}_6hourly_CC_OK/RH80_PR{PR}_20000day_startfrom_{start}day_final.dat\"\n",
    "#         # z_data = z_zonal_calc.load_data(file_path)\n",
    "#         # div_data = div_zonal_calc.load_data(file_path)\n",
    "        \n",
    "#         p_data = p_zonal_calc.load_data(file_path)\n",
    "#         # ps_data = ps_zonal_calc.load_data(file_path)\n",
    "\n",
    "#         # theta = t_data * (ps_data / p_data) ** (Rd / cp)\n",
    "#         # theta_prime = theta - theta.mean(axis=3)[:, :, :, np.newaxis]\n",
    "#         # z_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = z_data\n",
    "#         p_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = p_data\n",
    "#         # div_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = div_data\n",
    "        \n",
    "#         # theta_prime_final[i * num_time_steps:(i + 1) * num_time_steps, :, :, :] = theta_prime\n",
    "\n",
    "#         if start % 1000 == 0:\n",
    "#             print(f\"Processed day {start}\")\n",
    "\n",
    "#     # b_prime = theta_prime_final / np.mean(theta_final, axis=0) * 9.81\n",
    "#     # # Save results\n",
    "#     # b_prime_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/b_prime/PR{PR}_500_20000day_6hourly_b_prime.dat\"\n",
    "#     # with h5py.File(b_prime_file, 'w') as file:\n",
    "#     #     file.create_dataset('b_prime', data=b_prime)\n",
    "\n",
    "#     # Save results\n",
    "#     # z_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/z/PR{PR}_500_20000day_6hourly_z.dat\"\n",
    "#     p_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/qv/PR{PR}_500_20000day_6hourly_qv.dat\"\n",
    "#     with h5py.File(p_file, 'w') as file:\n",
    "#         file.create_dataset('qv', data=p_final)\n",
    "#     print(f\"Completed PR={PR}\")\n",
    "#     return f\"Completed PR={PR}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79021c8e-8a52-4013-ab07-f4d9bfeb91e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR=20\n",
      "Processed day 1000\n",
      "Processed day 2000\n",
      "Processed day 3000\n",
      "Processed day 4000\n",
      "Processed day 5000\n",
      "Processed day 6000\n",
      "Processed day 7000\n",
      "Processed day 8000\n",
      "Processed day 9000\n",
      "Processed day 10000\n",
      "Processed day 11000\n",
      "Processed day 12000\n",
      "Processed day 13000\n",
      "Processed day 14000\n",
      "Processed day 15000\n",
      "Processed day 16000\n",
      "Processed day 17000\n",
      "Processed day 18000\n",
      "Processed day 19000\n",
      "Completed PR=20\n",
      "All processes completed\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     PR_values = [0,10,20,30,40,50]\n",
    "#     with Pool(processes=6) as pool:  # Adjust the number of processes according to your system\n",
    "#         results = pool.map(process_pr, PR_values)\n",
    "#     print(\"All processes completed\")\n",
    "#     # for result in results:\n",
    "#         # print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3891dd91-1234-43a1-b2fd-1fb8ae4c75b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR=0\n",
      "<KeysViewHDF5 ['_types', 'convection_xyzt', 'factor1_xyzt', 'factor2_xyzt', 'factor3_xyzt', 'factor4_xyzt', 'grid_div_xyzt', 'grid_geopots_xyzt', 'grid_p_full_xyzt', 'grid_p_half_xyzt', 'grid_ps_c_xyzt', 'grid_ps_p_xyzt', 'grid_t_c_xyzt', 'grid_t_n_xyzt', 'grid_t_p_xyzt', 'grid_tracers_c_xyzt', 'grid_tracers_diff_xyzt', 'grid_tracers_n_xyzt', 'grid_tracers_p_xyzt', 'grid_u_c_xyzt', 'grid_u_n_xyzt', 'grid_u_p_xyzt', 'grid_v_c_xyzt', 'grid_v_n_xyzt', 'grid_v_p_xyzt', 'grid_vor_xyzt', 'grid_w_full_xyzt', 'grid_z_full_xyzt', 'grid_δtracers_xyzt', 'grid_δu_xyzt', 'grid_δv_xyzt', 'spe_div_c_xyzt', 'spe_div_p_xyzt', 'spe_lnps_c_xyzt', 'spe_lnps_p_xyzt', 'spe_t_c_xyzt', 'spe_t_p_xyzt', 'spe_tracers_c_xyzt', 'spe_tracers_p_xyzt', 'spe_vor_c_xyzt', 'spe_vor_p_xyzt']>\n"
     ]
    }
   ],
   "source": [
    "# PR = 0\n",
    "# from Zonal_Calculation import Zonal_Calculation\n",
    "\n",
    "# print(f\"PR={PR}\")\n",
    "# days = range(500, 525, 25)\n",
    "# num_days = len(days)\n",
    "# num_time_steps = 100  # Assuming each file contributes 100 time steps\n",
    "# z_dim, y_dim, x_dim = 20, 64, 128  # Dimensions of z, y, x\n",
    "\n",
    "# # Initialize Zonal_Calculation object once if all files have the same structure\n",
    "# z_zonal_calc = Zonal_Calculation(3, \"grid_div_xyzt\")\n",
    "# # p_zonal_calc = Zonal_Calculation(3, \"grid_p_full_xyzt\")\n",
    "# # ps_zonal_calc = Zonal_Calculation(3, \"grid_ps_c_xyzt\")\n",
    "\n",
    "# # Pre-allocate the entire array\n",
    "# # b_prime_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "# z_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "# # theta_prime_final = np.empty((num_days * num_time_steps, z_dim, y_dim, x_dim), dtype=np.float32)\n",
    "\n",
    "# Rd = 287\n",
    "# cp = 1004\n",
    "\n",
    "# # Process each day\n",
    "# for i, start in enumerate(days):\n",
    "#     file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/HSt42_{PR}_6hourly_CC_OK/RH80_PR{PR}_20000day_startfrom_{start}day_final.dat\"\n",
    "#     z_data = z_zonal_calc.load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a155fc-1686-4ea2-80aa-63fea9125d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6d574-ca2e-46e9-8817-9f1dcbd662e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387d3e3e-5cc7-4fcb-be8b-9972f5d6e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix_operation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771c625-521a-4ce7-8171-d9154786b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.decomposition import PCA\n",
    "# from netCDF4 import Dataset as NetCDFFile\n",
    "# import random\n",
    "# import timeit\n",
    "# import pickle\n",
    "# from scipy import stats\n",
    "# import xarray as xr\n",
    "# import pandas as pd\n",
    "# import statistics\n",
    "# from scipy import interpolate\n",
    "# import os\n",
    "# import pytz, datetime\n",
    "# import multiprocessing as mp\n",
    "# import h5py\n",
    "# from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b8a843-8e9a-416f-b6a7-d05d089c09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import h5py\n",
    "# from multiprocessing import Pool\n",
    "# import gc\n",
    "\n",
    "# # Saving dz\n",
    "# def process_theta(pr):\n",
    "#     print(f\"Processing PR={pr}\")\n",
    "#     theta_file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{pr}/z/PR{pr}_500_20000day_6hourly_z.dat\"\n",
    "#     with h5py.File(theta_file_path, 'r') as file:\n",
    "#         theta = np.asarray(file['dz'][:])\n",
    "\n",
    "#     theta_new = np.einsum('tzyx->txyz', theta)\n",
    "#     dim_t, dim_z, dim_y, dim_x = theta.shape\n",
    "#     del theta\n",
    "#     theta_new = np.reshape(theta_new, [dim_t * dim_x * dim_y, dim_z])\n",
    "\n",
    "#     L = np.eye(20)\n",
    "#     central_diff = np.roll(L, -1, axis=0) - np.roll(L, 1, axis=0)\n",
    "#     backward_diff = L - np.roll(L, -1, axis=1)  # for top boundary\n",
    "\n",
    "#     theta_central_diff = theta_new.dot(central_diff)\n",
    "#     theta_backward_diff = theta_new.dot(backward_diff)\n",
    "#     del theta_new\n",
    "#     theta_central_diff[:, 0]  = theta_backward_diff[:, 0]   # Use backward diff for first level\n",
    "#     theta_central_diff[:, 19] = theta_backward_diff[:, 18] # Fix index for last level\n",
    "\n",
    "#     theta_central_diff = theta_central_diff.reshape([dim_t, dim_x, dim_y, dim_z])\n",
    "#     theta_central_diff = np.einsum('txyz->tzyx', theta_central_diff)\n",
    "\n",
    "#     # Calculate mean ignoring time and x dimensions\n",
    "#     theta_diff_tx_mean = theta_central_diff.mean(axis=(0, 3))\n",
    "#     theta_central_diff_zonal_mean = theta_central_diff.mean(axis=3)\n",
    "#     # del theta_central_diff\n",
    "#     # gc.collect()\n",
    "\n",
    "#     # posi = np.where(theta_diff_tx_mean[18,:]<0)\n",
    "#     # theta_diff_tx_mean[18,posi] = theta_diff_tx_mean[17,posi]\n",
    "#     # posi = np.where(theta_diff_tx_mean[19,:]<0)\n",
    "#     # theta_diff_tx_mean[19,posi] = theta_diff_tx_mean[18,posi]\n",
    "\n",
    "#     # Fix negative values based on zonal means\n",
    "#     for i in range(78000):\n",
    "#         posi_x, posi_y = np.where(theta_central_diff_zonal_mean[i, :] < 0)\n",
    "#         theta_central_diff_zonal_mean[i, posi_x, posi_y] = theta_diff_tx_mean[posi_x, posi_y]\n",
    "#     del theta_diff_tx_mean\n",
    "#     gc.collect()\n",
    "#     # Saving\n",
    "#     dtheta_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{pr}/dz/PR{pr}_500_20000day_6hourly_dz_zonal_mean.dat\"\n",
    "#     with h5py.File(dtheta_file, 'w') as file:\n",
    "#         file.create_dataset('dz', data=theta_central_diff_zonal_mean)\n",
    "#     del theta_central_diff_zonal_mean\n",
    "\n",
    "#     dtheta_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{pr}/dz/PR{pr}_500_20000day_6hourly_dz.dat\"\n",
    "#     with h5py.File(dtheta_file, 'w') as file:\n",
    "#         file.create_dataset('dz', data=theta_central_diff)\n",
    "#     del theta_central_diff\n",
    "\n",
    "#     gc.collect()    \n",
    "#     print(f\"Completed PR={pr}\")\n",
    "#     # return theta_central_diff_zonal_mean\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     PR_values = [40]\n",
    "#     with Pool(processes=1) as pool:\n",
    "#         pool.map(process_theta, PR_values)\n",
    "#     print(\"All processes completed\")\n",
    "#     # for result in results:\n",
    "#         # print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5bc6ce9-3d0f-42ae-afd3-713a4a7d0663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PR=40\n",
      "Completed PR=40\n",
      "All processes completed\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import h5py\n",
    "# from multiprocessing import Pool\n",
    "# import gc\n",
    "\n",
    "# # Saving dtheta\n",
    "# def process_theta(pr):\n",
    "#     print(f\"Processing PR={pr}\")\n",
    "#     theta_file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{pr}/theta/PR{pr}_500_20000day_6hourly_theta.dat\"\n",
    "#     with h5py.File(theta_file_path, 'r') as file:\n",
    "#         theta = np.asarray(file['theta'][:])\n",
    "\n",
    "#     theta_new = np.einsum('tzyx->txyz', theta)\n",
    "#     dim_t, dim_z, dim_y, dim_x = theta.shape\n",
    "#     del theta\n",
    "#     theta_new = np.reshape(theta_new, [dim_t * dim_x * dim_y, dim_z])\n",
    "\n",
    "#     L = np.eye(20)\n",
    "#     central_diff = np.roll(L, -1, axis=0) - np.roll(L, 1, axis=0)\n",
    "#     backward_diff = L - np.roll(L, -1, axis=1)  # for top boundary\n",
    "\n",
    "#     theta_central_diff = theta_new.dot(central_diff)\n",
    "#     theta_backward_diff = theta_new.dot(backward_diff)\n",
    "#     del theta_new\n",
    "#     theta_central_diff[:, 0]  = theta_backward_diff[:, 0]   # Use backward diff for first level\n",
    "#     theta_central_diff[:, 19] = theta_backward_diff[:, 18] # Fix index for last level\n",
    "\n",
    "#     theta_central_diff = theta_central_diff.reshape([dim_t, dim_x, dim_y, dim_z])\n",
    "#     theta_central_diff = np.einsum('txyz->tzyx', theta_central_diff)\n",
    "\n",
    "#     # Calculate mean ignoring time and x dimensions\n",
    "#     theta_diff_tx_mean = theta_central_diff.mean(axis=(0, 3))\n",
    "#     theta_central_diff_zonal_mean = theta_central_diff.mean(axis=3)\n",
    "#     # del theta_central_diff\n",
    "#     gc.collect()\n",
    "\n",
    "#     # posi = np.where(theta_diff_tx_mean[18,:]<0)\n",
    "#     # theta_diff_tx_mean[18,posi] = theta_diff_tx_mean[17,posi]\n",
    "#     # posi = np.where(theta_diff_tx_mean[19,:]<0)\n",
    "#     # theta_diff_tx_mean[19,posi] = theta_diff_tx_mean[18,posi]\n",
    "\n",
    "#     # Fix negative values based on zonal means\n",
    "#     for i in range(78000):\n",
    "#         posi_x, posi_y = np.where(theta_central_diff_zonal_mean[i, :] < 0)\n",
    "#         theta_central_diff_zonal_mean[i, posi_x, posi_y] = theta_diff_tx_mean[posi_x, posi_y]\n",
    "#     del theta_diff_tx_mean\n",
    "#     gc.collect()\n",
    "#     # Saving\n",
    "#     dtheta_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{pr}/dtheta/PR{pr}_500_20000day_6hourly_dtheta_zonal_mean.dat\"\n",
    "#     with h5py.File(dtheta_file, 'w') as file:\n",
    "#         file.create_dataset('dtheta', data=theta_central_diff_zonal_mean)\n",
    "#     del theta_central_diff_zonal_mean\n",
    "\n",
    "#     dtheta_file = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{pr}/dtheta/PR{pr}_500_20000day_6hourly_dtheta.dat\"\n",
    "#     with h5py.File(dtheta_file, 'w') as file:\n",
    "#         file.create_dataset('dtheta', data=theta_central_diff)\n",
    "#     del theta_central_diff\n",
    "\n",
    "#     gc.collect()    \n",
    "#     print(f\"Completed PR={pr}\")\n",
    "#     # return theta_central_diff_zonal_mean\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     PR_values = [40]\n",
    "#     with Pool(processes=1) as pool:\n",
    "#         pool.map(process_theta, PR_values)\n",
    "#     print(\"All processes completed\")\n",
    "#     # for result in results:\n",
    "#         # print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e53318-9d4f-4301-a539-82f98fd9495a",
   "metadata": {},
   "source": [
    "# Transform dat to nc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "480dad5c-0b51-4741-a6f6-095980ed0ad0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     output_nc_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/prec/PR\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_500_20000day_6hourly_prec.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Save the Dataset to a NetCDF file\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_nc_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprec_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_nc_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll files converted successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML22/lib/python3.12/site-packages/xarray/core/dataset.py:2329\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   2326\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[0;32m-> 2329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML22/lib/python3.12/site-packages/xarray/backends/api.py:1376\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multifile \u001b[38;5;129;01mand\u001b[39;00m compute:\n\u001b[0;32m-> 1376\u001b[0m         \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compute:\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML22/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:577\u001b[0m, in \u001b[0;36mNetCDF4DataStore.close\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML22/lib/python3.12/site-packages/xarray/backends/file_manager.py:234\u001b[0m, in \u001b[0;36mCachingFileManager.close\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    232\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key, default)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import h5py\n",
    "# import xarray as xr\n",
    "\n",
    "# # List of PR values as given in your code\n",
    "# PR_values = [0, 10, 20, 30, 40, 50]\n",
    "\n",
    "# for PR in PR_values:\n",
    "#     # Construct the file path\n",
    "#     prec_file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/prec/PR{PR}_500_20000day_6hourly_prec_0_300hPa_sum.dat\"\n",
    "    \n",
    "#     # Read the data from the .dat file using h5py\n",
    "#     with h5py.File(prec_file_path, 'r') as file:\n",
    "#         # Assuming 'prec' is the dataset name in the HDF5 file\n",
    "#         prec_data = np.asarray(file['prec'][:, :, :])  # Shape (time, lat, lon)\n",
    "\n",
    "#     # Define coordinates for the NetCDF file\n",
    "#     time = np.arange(prec_data.shape[0])  # Assuming time is sequential indices\n",
    "#     lat = np.linspace(-90, 90, prec_data.shape[1])   # Latitude from -90 to 90 degrees\n",
    "#     lon = np.linspace(0, 360, prec_data.shape[2])   # Longitude from 0 to 360 degrees\n",
    "\n",
    "#     # Convert to xarray Dataset\n",
    "#     ds = xr.Dataset(\n",
    "#         {\n",
    "#             \"prec\": ((\"time\", \"lat\", \"lon\"), prec_data)\n",
    "#         },\n",
    "#         coords={\n",
    "#             \"time\": time,\n",
    "#             \"lat\": lat,\n",
    "#             \"lon\": lon\n",
    "#         },\n",
    "#         attrs={\n",
    "#             'description': 'Precipitation data converted from .dat to .nc format',\n",
    "#             'units': 'mm/day'  # Example unit, adjust according to your data\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     # Define output NetCDF file path\n",
    "#     output_nc_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF/PR{PR}/prec/PR{PR}_500_20000day_6hourly_prec_0_300hPa_sum.nc\"\n",
    "    \n",
    "#     # Save the Dataset to a NetCDF file\n",
    "#     ds.to_netcdf(output_nc_path)\n",
    "#     print(f\"Converted {prec_file_path} to {output_nc_path}\")\n",
    "\n",
    "# print(\"All files converted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a881d64-b26b-4031-a3ae-d352f733de3d",
   "metadata": {},
   "source": [
    "# To to u'D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50d5b27b-d833-4fff-9ced-275431e86aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import h5py\n",
    "# import os\n",
    "# import gc\n",
    "# from multiprocessing import Pool\n",
    "# import time\n",
    "\n",
    "# def process_pr(PR):\n",
    "#     print(f\"Processing PR={PR}\")\n",
    "#     start_time = time.time()\n",
    "#     days = range(500, 20000, 25)\n",
    "#     num_days = len(days)\n",
    "#     num_time_steps = 100  # Assuming each file contributes 100 time steps\n",
    "#     z_dim, y_dim, x_dim = 20, 64, 128  # Dimensions of z, y, x\n",
    "\n",
    "#     # Initialize Zonal Calculators\n",
    "#     div_zonal_calc = Zonal_Calculation(3, \"grid_div_xyzt\")\n",
    "#     u_zonal_calc   = Zonal_Calculation(3, \"grid_u_c_xyzt\")\n",
    "#     print(\"---Initialized Zonal Calculators---\")\n",
    "\n",
    "#     # Memory-mapped arrays for the final datasets\n",
    "#     u_prime_final = np.memmap(f'/data92/PeterChang/u_prime_final_PR{PR}.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     div_prime_final = np.memmap(f'/data92/PeterChang/div_prime_final_PR{PR}.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "    \n",
    "#     # Process each day in chunks\n",
    "#     chunk_size = 1000  # Adjust chunk size as needed\n",
    "#     for i, start_day in enumerate(days):\n",
    "#         file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/HSt42_{PR}_6hourly_CC_OK/RH80_PR{PR}_20000day_startfrom_{start_day}day_final.dat\"\n",
    "        \n",
    "#         # Process data in smaller chunks to reduce memory usage\n",
    "#         for chunk_start in range(0, num_time_steps, chunk_size):\n",
    "#             chunk_end = min(chunk_start + chunk_size, num_time_steps)\n",
    "\n",
    "#             # Load data\n",
    "#             u_data = u_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "#             div_data = div_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "\n",
    "#             # Calculate anomalies\n",
    "#             u_prime_chunk = u_zonal_calc.zonal_anomaly(u_data)\n",
    "#             div_prime_chunk = div_data - div_data.mean(axis=3, keepdims=True)\n",
    "\n",
    "#             # Save the data in the memory-mapped arrays\n",
    "#             u_prime_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = u_prime_chunk\n",
    "#             div_prime_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = div_prime_chunk\n",
    "\n",
    "#             # Clear memory\n",
    "#             del u_data, div_data, u_prime_chunk, div_prime_chunk\n",
    "#             gc.collect()\n",
    "\n",
    "#         if (i + 1) % 5 == 0:\n",
    "#             print(f\"Processed up to day {start_day} for PR={PR}\")\n",
    "#         gc.collect()\n",
    "    \n",
    "#     # Calculate -(u_prime_final * div_prime_final)\n",
    "#     uD_final = -(u_prime_final * div_prime_final)\n",
    "\n",
    "#     # Save the result to HDF5\n",
    "#     common_path = \"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF\"\n",
    "#     uD_file = f\"{common_path}/PR{PR}/uD/PR{PR}_500_20000day_6hourly_uD.dat\"\n",
    "    \n",
    "#     save_to_hdf5(uD_final, '-uD', uD_file)\n",
    "\n",
    "#     # Clean up memory-mapped arrays\n",
    "#     del u_prime_final, div_prime_final, uD_final\n",
    "#     gc.collect()\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     print(f\"Execution time for PR={PR}: {end_time - start_time:.2f} seconds\")\n",
    "#     print(f\"Completed processing for PR={PR}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c36fa8-8cd6-43bb-8503-ef64dbb0969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_to_hdf5(data, data_name, file_path):\n",
    "#     with h5py.File(file_path, 'w') as file:\n",
    "#         file.create_dataset(data_name, data=data)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     PR_values = [0, 10, 20, 30, 40, 50]\n",
    "#     with Pool(processes=6) as pool:\n",
    "#         pool.map(process_pr, PR_values)\n",
    "#     print(\"All processes completed\")\n",
    "\n",
    "# print(\"Basic variables saving done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5b404f-21e5-4a30-a779-25e15b345e5b",
   "metadata": {},
   "source": [
    "# Save vor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d622d2-5a78-4250-be3a-d26df514e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import h5py\n",
    "# import os\n",
    "# import gc\n",
    "# from multiprocessing import Pool\n",
    "# import time\n",
    "\n",
    "# def process_pr(PR):\n",
    "#     print(f\"Processing PR={PR}\")\n",
    "#     start_time = time.time()\n",
    "#     days = range(500, 20000, 25)\n",
    "#     num_days = len(days)\n",
    "#     num_time_steps = 100  # Assuming each file contributes 100 time steps\n",
    "#     z_dim, y_dim, x_dim = 20, 64, 128  # Dimensions of z, y, x\n",
    "\n",
    "#     # Initialize Zonal Calculators\n",
    "#     vor_zonal_calc = Zonal_Calculation(3, \"grid_vor_xyzt\")\n",
    "#     print(\"---Initialized Zonal Calculators---\")\n",
    "\n",
    "#     # Memory-mapped arrays for the final datasets\n",
    "#     vor_final = np.memmap(f'/data92/PeterChang/vor_final_PR{PR}.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "#     vor_prime_final = np.memmap(f'/data92/PeterChang/vor_prime_final_PR{PR}.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "    \n",
    "#     # div_prime_final = np.memmap(f'/data92/PeterChang/div_prime_final_PR{PR}.dat', dtype=np.float32, mode='w+', shape=(num_days * num_time_steps, z_dim, y_dim, x_dim))\n",
    "    \n",
    "#     # Process each day in chunks\n",
    "#     chunk_size = 1000  # Adjust chunk size as needed\n",
    "#     for i, start_day in enumerate(days):\n",
    "#         file_path = f\"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/HSt42_{PR}_6hourly_CC_OK/RH80_PR{PR}_20000day_startfrom_{start_day}day_final.dat\"\n",
    "        \n",
    "#         # Process data in smaller chunks to reduce memory usage\n",
    "#         for chunk_start in range(0, num_time_steps, chunk_size):\n",
    "#             chunk_end = min(chunk_start + chunk_size, num_time_steps)\n",
    "\n",
    "#             # Load data\n",
    "#             vor_data = vor_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "#             # div_data = div_zonal_calc.load_data(file_path)[chunk_start:chunk_end]\n",
    "\n",
    "#             # Calculate anomalies\n",
    "#             vor_chunk = vor_zonal_calc.zonal_anomaly(vor_data)\n",
    "#             vor_prime_chunk = vor_data - vor_data.mean(axis=3, keepdims=True)\n",
    "\n",
    "#             # Save the data in the memory-mapped arrays\n",
    "#             vor_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = vor_chunk\n",
    "#             vor_prime_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = vor_prime_chunk\n",
    "            \n",
    "#             # div_prime_final[i * num_time_steps + chunk_start : i * num_time_steps + chunk_end] = div_prime_chunk\n",
    "\n",
    "#             # Clear memory\n",
    "#             # del u_data, div_data, u_prime_chunk, div_prime_chunk\n",
    "#             del vor_data, vor_chunk, vor_prime_chunk\n",
    "            \n",
    "#             gc.collect()\n",
    "\n",
    "#         if (i + 1) % 5 == 0:\n",
    "#             print(f\"Processed up to day {start_day} for PR={PR}\")\n",
    "#         gc.collect()\n",
    "    \n",
    "#     # Calculate -(u_prime_final * div_prime_final)\n",
    "#     # uD_final = -(u_prime_final * div_prime_final)\n",
    "\n",
    "#     # Save the result to HDF5\n",
    "#     common_path = \"/data92/PeterChang/back_to_master1220/Moist_Dycore/IdealizeSpetral.jl/exp/HSt42/6hourly_uv_prime_EMF\"\n",
    "#     vor_file = f\"{common_path}/PR{PR}/vor/PR{PR}_500_20000day_6hourly_vor.dat\"\n",
    "#     vor_prime_file = f\"{common_path}/PR{PR}/vor_prime/PR{PR}_500_20000day_6hourly_vor_prime.dat\"\n",
    "    \n",
    "    \n",
    "#     save_to_hdf5(vor_final, 'vor', vor_file)\n",
    "#     save_to_hdf5(vor_prime_final, 'vor_prime', vor_prime_file)\n",
    "    \n",
    "\n",
    "#     # Clean up memory-mapped arrays\n",
    "#     # del u_prime_final, div_prime_final, uD_final\n",
    "#     del vor_final, vor_prime_final\n",
    "#     gc.collect()\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     print(f\"Execution time for PR={PR}: {end_time - start_time:.2f} seconds\")\n",
    "#     print(f\"Completed processing for PR={PR}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba8bd4f-c610-4a2b-95b9-959b9020e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_to_hdf5(data, data_name, file_path):\n",
    "#     with h5py.File(file_path, 'w') as file:\n",
    "#         file.create_dataset(data_name, data=data)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     PR_values = [0, 10, 20, 30, 40, 50]\n",
    "#     with Pool(processes=6) as pool:\n",
    "#         pool.map(process_pr, PR_values)\n",
    "#     print(\"All processes completed\")\n",
    "\n",
    "# print(\"Basic variables saving done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
